{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b155cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import T5Tokenizer, MT5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import WebNLGDataset\n",
    "\n",
    "# Google's Official Preprocess Codes\n",
    "# https://github.com/google-research/language/blob/master/language/totto/baseline_preprocessing/preprocess_utils.py\n",
    "from preprocess_utils import get_highlighted_subtable, linearize_subtable\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5494ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Config\n",
    "device=torch.device('cpu')\n",
    "lr=1e-4\n",
    "batch_size=4 # 3 for 't5-large' and make 'accumulation_steps' larger\n",
    "accumulation_steps=3\n",
    "epochs=10\n",
    "LOW_RESOURCE_SIZE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c805a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(250100, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-Trained T5 Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/mt5-base')\n",
    "# Add Special Tokens: Table Tags\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [\n",
    "        '|',\n",
    "        ':',\n",
    "    ]\n",
    "})\n",
    "# Pre-Trained T5 Model\n",
    "model = MT5ForConditionalGeneration.from_pretrained('google/mt5-base').to(device)\n",
    "# Resize PLM's Embedding Layer\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcae3470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized English train dataloader with 3658 samples.\n",
      "Initialized English dev dataloader with 198 samples.\n"
     ]
    }
   ],
   "source": [
    "dataset_train_en = WebNLGDataset(tokenizer=tokenizer, language='en')\n",
    "\n",
    "if LOW_RESOURCE_SIZE > 0:\n",
    "    sampled_train_indices = np.linspace(0, len(dataset_train)-1, num=LOW_RESOURCE_SIZE, dtype=int)\n",
    "    subset_train = torch.utils.data.Subset(dataset_train, sampled_indices)\n",
    "    dataloader_train_en = DataLoader(subset_train, batch_size=batch_size, shuffle=True, collate_fn=dataset_train_en.collate_fn)\n",
    "else:\n",
    "    dataloader_train_en = DataLoader(dataset_train_en, batch_size=batch_size, shuffle=True, collate_fn=dataset_train_en.collate_fn)\n",
    "    \n",
    "dataset_dev_en = WebNLGDataset(tokenizer=tokenizer, language='en', split='dev')\n",
    "dataloader_dev_en = DataLoader(dataset_dev_en, batch_size=batch_size, shuffle=False, collate_fn=dataset_dev_en.collate_fn)\n",
    "\n",
    "print(f'Initialized English train dataloader with {len(dataloader_train_en)} samples.')\n",
    "print(f'Initialized English dev dataloader with {len(dataloader_dev_en)} samples.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff392da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Russian train dataloader with 3658 samples.\n",
      "Initialized Russian dev dataloader with 198 samples.\n"
     ]
    }
   ],
   "source": [
    "dataset_train_ru = WebNLGDataset(tokenizer=tokenizer, language='ru')\n",
    "dataloader_train_ru = DataLoader(dataset_train_ru, batch_size=batch_size, shuffle=True, collate_fn=dataset_train_ru.collate_fn)\n",
    "dataset_dev_ru = WebNLGDataset(tokenizer=tokenizer, language='ru', split='dev')\n",
    "dataloader_dev_ru = DataLoader(dataset_dev_ru, batch_size=batch_size, shuffle=False, collate_fn=dataset_dev_ru.collate_fn)\n",
    "\n",
    "print(f'Initialized Russian train dataloader with {len(dataloader_train_ru)} samples.')\n",
    "print(f'Initialized Russian dev dataloader with {len(dataloader_dev_ru)} samples.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "908a90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "with open(\"../webnlg_data/references/reference-en0\", \"r\") as reference_file:\n",
    "    for line in reference_file:\n",
    "        stripped_line = line.strip().split()\n",
    "        references.append(stripped_line)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8161b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, epoch):\n",
    "    \n",
    "    # PLM (Eval Mode)\n",
    "    model.eval()\n",
    "\n",
    "    # Trained Model\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_blue = 0\n",
    "    with torch.no_grad():\n",
    "        for language, dataloader in [('english', dataloader_dev_en), ('russian', dataloader_dev_ru)]:\n",
    "            \n",
    "            # Generation\n",
    "            if os.path.exists(f'../webnlg_data/multilingual_fine_tuning/{language}_epoch{epoch}_lowsize{LOW_RESOURCE_SIZE}.txt'):\n",
    "                os.remove(f'../webnlg_data/multilingual_fine_tuning/{language}_epoch{epoch}_lowsize{LOW_RESOURCE_SIZE}.txt')\n",
    "            f = open(f'../webnlg_data/multilingual_fine_tuning/{language}_epoch{epoch}_lowsize{LOW_RESOURCE_SIZE}.txt', 'a')\n",
    "            \n",
    "            for idx, (data, attn_mask, _) in enumerate(dataloader):\n",
    "                if (idx+1)%100==0: print(batch_size*(idx+1), 'generated')\n",
    "\n",
    "                data=data.to(device)\n",
    "                attn_mask=attn_mask.to(device)\n",
    "\n",
    "                # Beam Search\n",
    "                outputs = model.generate(\n",
    "                    data,\n",
    "                    max_length=300,\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "\n",
    "                blue_score = 0\n",
    "                for idx2, generation in enumerate(tokenizer.batch_decode(outputs, skip_special_tokens=True)):\n",
    "                    reference = references[idx + idx2]\n",
    "                    candidate = generation.strip().split()\n",
    "                    blue_score += sentence_bleu(reference, candidate) / batch_size\n",
    "                    f.write(generation + '\\n')\n",
    "                total_blue += blue_score\n",
    "                if idx == 1:\n",
    "                    break\n",
    "\n",
    "            f.close()\n",
    "\n",
    "            f = open(f'../webnlg_data/multilingual_fine_tuning/blue_scores_{language}_lowsize{LOW_RESOURCE_SIZE}.txt', 'a')\n",
    "            f.write('BLUE after ' + str(epoch) + ' epochs: ' + str(blue_score) + '\\n')\n",
    "            f.close()\n",
    "    \n",
    "    return total_blue / len(dataloader_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6525b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidvos/Developer/Prefix Tuning/mt5-prefix-tuning/transformers/src/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optim, Scheduler\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler=get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1000,\n",
    "    num_training_steps=int(epochs*(len(dataloader_train_en)+len(dataloader_train_ru))/(accumulation_steps*batch_size))\n",
    ")\n",
    "\n",
    "# TensorBoard: Logging\n",
    "writer=SummaryWriter()\n",
    "step_global=0\n",
    "\n",
    "best_bleu_score = 0\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train Phase\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_train=0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for language, dataloader in [('english', dataloader_train_en), ('russian', dataloader_train_ru)]:\n",
    "        for step, (data, attn_mask, label) in enumerate(dataloader):\n",
    "            data=data.to(device)\n",
    "            attn_mask=attn_mask.to(device)\n",
    "            label=label.to(device)\n",
    "\n",
    "            outputs=model(input_ids=data, attention_mask=attn_mask, labels=label)\n",
    "\n",
    "            loss=outputs[0]/accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            loss_train+=loss.item()\n",
    "\n",
    "            if (step+1)%accumulation_steps==0:\n",
    "                step_global+=1\n",
    "\n",
    "                # TensorBoard\n",
    "                writer.add_scalar(\n",
    "                    f'loss_train/MT5-base_Fine-Tuning_lr{lr}_batch{int(accumulation_steps*batch_size)}_epoch{epochs}',\n",
    "                    loss_train,\n",
    "                    step_global\n",
    "                )\n",
    "                # Console\n",
    "                if step_global%1000==0:\n",
    "                    print(f'epoch {epoch+1} step {step_global} loss_train {loss_train:.4f}')\n",
    "                # Set Loss to 0\n",
    "                loss_train=0\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "    blue_score = evaluate_model(model, epoch)\n",
    "    print(f'BLUE of {blue_score}')\n",
    "    \n",
    "    if blue_score > best_bleu_score:\n",
    "        best_model = model.deepcopy()\n",
    "        best_model_name = f'MT5-base_Fine-Tuning_lr{lr}_batch{int(accumulation_steps*batch_size)}_lowsize{LOW_RESOURCE_SIZE}_epoch{epoch+1}of{epochs}.pt'\n",
    "        best_bleu_score = blue_score\n",
    "        \n",
    "best_model.to(torch.device('cpu'))\n",
    "torch.save(best_model, f'../model/{best_model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8cb23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
