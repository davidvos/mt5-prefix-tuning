{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f391639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import T5Tokenizer, MT5ForConditionalGeneration\n",
    "\n",
    "from preprocess_utils import get_highlighted_subtable, linearize_subtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb6f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cpu')\n",
    "batch_size=4 # 10 for 't5-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b788cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Trained T5 Tokenizer\n",
    "tokenizer=T5Tokenizer.from_pretrained('google/mt5-base')\n",
    "# Add Special Tokens: Table Tags\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [\n",
    "        '|',\n",
    "        ':'\n",
    "    ]\n",
    "})\n",
    "# Pre-Trained T5 Model\n",
    "pretrained = MT5ForConditionalGeneration.from_pretrained('google/mt5-base').to(device)\n",
    "# Resize PLM's Embedding Layer\n",
    "pretrained.resize_token_embeddings(len(tokenizer))\n",
    "# Freeze LM\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08937c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebNLGDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, raw_path='../webnlg_data/release_v3.0/ru', language='en', data_path='../webnlg_data/preprocessed', split='train'):\n",
    "        \n",
    "        if not os.path.exists(f'{data_path}/{split}.json'):\n",
    "            b = Benchmark()\n",
    "            files = select_files(raw_path)\n",
    "            b.fill_benchmark(files)\n",
    "            b.b2json(data_path, f'{split}.json')\n",
    "        \n",
    "        with open(f'{data_path}/{split}.json', 'r') as f:\n",
    "            dataset = json.load(f)\n",
    "            entries = dataset['entries']\n",
    "\n",
    "        full_rela_lst = []\n",
    "        full_src_lst = []\n",
    "        full_tgt_lst = []\n",
    "        for i, entry in enumerate(entries):\n",
    "            sents = entry[str(i + 1)]['lexicalisations']\n",
    "            triples = entry[str(i + 1)]['modifiedtripleset']\n",
    "            \n",
    "            rela_lst = []\n",
    "            temp_triples = ''\n",
    "            for j, tripleset in enumerate(triples):\n",
    "                subj, rela, obj = tripleset['subject'], tripleset['property'], tripleset['object']\n",
    "                rela_lst.append(rela)\n",
    "                temp_triples += ' | '\n",
    "                temp_triples += '{} : {} : {}'.format(subj, rela, obj)\n",
    "\n",
    "            for sent in sents:\n",
    "                if sent[\"lang\"] == language:\n",
    "                    full_tgt_lst.append(sent[\"lex\"])\n",
    "                    full_src_lst.append(temp_triples)\n",
    "                    full_rela_lst.append(rela_lst)\n",
    "                    if split == 'dev':\n",
    "                        break\n",
    "            \n",
    "        assert len(full_rela_lst) == len(full_src_lst)\n",
    "        assert len(full_rela_lst) == len(full_tgt_lst)\n",
    "\n",
    "        self.examples = []\n",
    "        self.targets = []\n",
    "        for src, tgt in zip(full_src_lst, full_tgt_lst):\n",
    "            src = tokenizer.encode(src)\n",
    "            if len(src)>512:\n",
    "                # Truncate\n",
    "                encoded = src[:511] + [tokenizer.eos_token_id]\n",
    "            self.examples.append(src)\n",
    "    \n",
    "            tgt = tokenizer.encode(tgt)\n",
    "            self.targets.append(tgt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "772706bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Same Sequence Length on Same Batch\n",
    "    \"\"\"\n",
    "    max_len_data=0\n",
    "    for data in batch:\n",
    "        if len(data)>max_len_data: max_len_data=len(data)\n",
    "            \n",
    "    datas=[]\n",
    "    attn_masks=[]\n",
    "    for data in batch:\n",
    "        data.extend([tokenizer.pad_token_id]*(max_len_data-len(data)))\n",
    "        datas.append(data)\n",
    "        \n",
    "        attn_mask=[int(e!=tokenizer.pad_token_id) for e in data]\n",
    "        attn_masks.append(attn_mask)\n",
    "        \n",
    "    return torch.tensor(datas), torch.tensor(attn_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4eef303",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dev = WebNLGDataset(tokenizer=tokenizer, split='dev')\n",
    "dataloader_dev = DataLoader(dataset_dev, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0afab367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTuning(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_config, prompt_len=20, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Config of Pre-Trained LM\n",
    "        self.pretrained_config=pretrained_config\n",
    "        \n",
    "        # torch.tensor([0, 1, 2, .. , prompt_len-1])\n",
    "        self.pre_prompt=torch.arange(prompt_len)\n",
    "        # Embedding\n",
    "        self.embd=nn.Embedding(num_embeddings=prompt_len, embedding_dim=pretrained_config.d_model)\n",
    "        # Reparameterization\n",
    "        self.reparam=nn.Sequential(\n",
    "            nn.Linear(pretrained_config.d_model, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, pretrained_config.d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch_size, device):\n",
    "        # Shape: batch_size, prompt_len\n",
    "        prompt=self.pre_prompt.unsqueeze(0).expand(batch_size, -1).to(device)\n",
    "        # Shape: batch_size, prompt_len, d_model\n",
    "        prompt=self.embd(prompt)\n",
    "        # Shape: batch_size, prompt_len, d_model\n",
    "        prompt=self.reparam(prompt)\n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e183f0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# PLM (Eval Mode)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpretrained\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Trained Model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../model/MT5-base_Prompt-Tuning_prompt-len100_hidden-dim768_lr0.3_batch8_epoch15of20.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained' is not defined"
     ]
    }
   ],
   "source": [
    "# PLM (Eval Mode)\n",
    "pretrained.eval()\n",
    "\n",
    "# Trained Model\n",
    "model=torch.load('../model/MT5-base_Prompt-Tuning_prompt-len100_hidden-dim768_lr0.3_batch8_epoch15of20.pt')\n",
    "model=model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Generation\n",
    "if os.path.exists('../webnlg_data/generation_dev_prefix_tuning.txt'):\n",
    "    os.remove('../webnlg_data/generation_dev_prefix_tuning.txt')\n",
    "f=open('../webnlg_data/generation_dev_prefix_tuning.txt', 'a')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (data, attn_mask) in enumerate(dataloader_dev):\n",
    "        if (idx+1)%100==0: print(batch_size*(idx+1), 'generated')\n",
    "            \n",
    "        data=data.to(device)\n",
    "        attn_mask=attn_mask.to(device)\n",
    "        \n",
    "        # Get Prompt\n",
    "        prompt=model(batch_size=data.shape[0], device=device)\n",
    "        \n",
    "        # Beam Search\n",
    "        outputs=pretrained.generate(\n",
    "            data,\n",
    "            max_length=300,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "            prompt=prompt\n",
    "        )\n",
    "        \n",
    "        for generation in tokenizer.batch_decode(outputs, skip_special_tokens=True):\n",
    "            f.write(generation+'\\n')\n",
    "            \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "!cd ../language_repo/ && bash language/totto/totto_eval.sh --prediction_path ../totto_data/generation_dev_prompt_tuning.txt --target_path ../totto_data/totto_dev_data.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20013e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
